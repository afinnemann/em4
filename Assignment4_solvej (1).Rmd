---
title: "Assignment 4 - Applying meta-analytic priors"
author: "Riccardo Fusaroli"
date: "3/16/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)
p_load(ggplot2,rethinking,brms,readxl,rstan,tidyverse,coda,mvtnorm,devtools)

setwd("C:/Users/Bruger/Google Drev/Cognitive Science/Experimental methods 4/Portfolio 4")

Data_1 <- read_excel("Assignment4MetaData.xlsx")
Data_2 <- read_excel("Assignment4PitchDatav2.xlsx")

str(Data_1)
Data_1$StudyID <- as.factor(Data_1$StudyID)
Data_1$Year_publication <- as.factor(Data_1$Year_publication)

str(Data_2)
Data_2$ID <- as.factor(Data_2$ID)

```

## Assignment 4

In this assignment we do the following:
- we reproduce the meta-analysis of pitch SD from last semester in a Bayesian framework
- we reproduce the pitch SD in schizophrenia analysis from last semester using both a conservative and a meta-analytic prior
- we assess the difference in model quality and estimates using the two priors.

The questions you need to answer are: What are the consequences of using a meta-analytic prior? Evaluate the models with conservative and meta-analytic priors. Discuss the effects on estimates. Discuss the effects on model quality. Discuss the role that meta-analytic priors should have in scientific practice. Should we systematically use them? Do they have drawbacks? Should we use them to complement more conservative approaches? How does the use of meta-analytic priors you suggest reflect the skeptical and cumulative nature of science?


### Step by step suggestions

Step 1: Reproduce the meta-analysis of pitch sd from previous studies of voice in schizophrenia
- the data is available as Assignment4MetaData.xlsx
- Effect size (cohen's d), sd and variance are already calculated (you're welcome!)
- Since we're only interested in getting a meta-analytic effect size, let's take a shortcut and use bromance magic (brms): https://mvuorre.github.io/post/2016/2016-09-29-bayesian-meta-analysis/

```{r}
#mean ES: mean effect size in cohens d (measure of effect size which is independend of measurement scale which the study have used. Some studies use Hz, some in ST (semi-tone). Cohens d, scale free - sd measure)
# we want to find a prior 
#bromance package BMR (glm,glmer,lm)
#simplest model: M0 <- BRM(meanES ~ 1, data)
# include random effect - allow for intercapt variation (pooling -> extreme values are pulled towards average. recalculate new mean after pooling): M0 <- BRM(meanES ~ 1 + (1|StudyID), data)
# additional effects: 
  #Prior: makes no sense 
  #Data: ....
  #Cores: 2 (something with the computer and speed)
  #Chain: 2
  #lTER: 2000 (try to find the write solution 2000 times)

M <- brms::brm(MeanES|se(SdES)~1+(1|StudyID),data = Data_1,cores = 2,chains = 2,iter = 2000)

summary(M)
#sd(intercept) group: percentile interval (distribution of residuals) the differerence in the data. and the uncertainty of this percentile interval.  
#intercept population: ES -0.60 sd(0.32), mean after pooling and sd of it. what we use for prior 

```

Step 2: Prepare the pitch SD data from last year
- the data is available as Assignment4PitchData.csv (thanks Celine)
- We do not know how to build random effects, yet. So we need to simplify the dataset to avoid interdependence between datapoint: How?
- Also, let's standardize the data, so that they are compatible with our meta-analytic prior (Cohen's d is measured in SDs).

```{r}
#scale data, standardize (data - mean / sd): PitchSD - Mean(PitchSD) / sd(PitchSD)
#mean of each participant 

Data_3 <- Data_2 %>% 
  mutate(sc_pitch = ((PitchSD - mean(PitchSD)) / sd(PitchSD))) %>% 
           group_by(ID_unique) %>% 
           summarise(PitchSD = mean(sc_pitch), 
                     diagnosis = diagnosis[1]) 
Data_3 <- as.data.frame(Data_3)  


str(Data_3)
Data_3$diagnosis <- as.numeric(Data_3$diagnosis)


```


Step 3: Build a regression model predicting Pitch SD from Diagnosis.
- how is the outcome distributed? (likelihood function)
- how are the parameters of the likelihood distribution distributed? Which predictors should they be conditioned on?
- use a skeptical/conservative prior for the effects of diagnosis. Remember you'll need to motivate it.
- Describe and plot the estimates. Evaluate model quality

```{r}
# normal in many natural instances (continues variables) -> when we no nothing reasonable to assume 
# we are finding means -> in continous sampling, mean is normally distributed?
# prior for a, normal(0,1). 0 because it it stadardized with mean 0. 1 because it allows the data to vary as much as the original data. 
# prior for b, normal(0,1).
# prior for sigma. either cauchy(0,2). expect lower deviance to be more likely, but high also possible.  
# alternative prior for sigma. log(sigma)~a_s + b_s * diagnosis. because the error (variance) is higher for patients with broad spectra disorders, than it is for control. log(sigma) because log is from 0 and positive to infinite - sigma (error) is always positive, and it is continous. 
# partial pooling: all participnts are different, but we can learn about the single participant from all the others - by pooling. Gives more realistic evidence. 
# mixed effect models as baysian: mu=a[participant]+b[participant]*diagnosis


# conservative priors
M_1 <- rethinking::map(
  alist(
    PitchSD ~ dnorm(mu,sigma),
    mu <- a + bD * diagnosis,
    a ~ dnorm(0,1),
    bD ~ dnorm(0,1),
    sigma ~ dcauchy(0,2)
    ),
  data=Data_3,
  start = list(a=mean(Data_3$PitchSD),bD=0,sigma=sd(Data_3$PitchSD)))

# print results
precis(M_1)
plot(precis(M_1))


# random effects
#M_3 <- rethinking::map(
#  alist(
#    PitchSD ~ dnorm(mu,sigma),
#    mu <- a[Data_2$ID]+b[Data_2$ID]*Data_2$diagnosis
#    a[ID] ~ dnorm(a_group,1),
#    a_group ~ dnorm(a_diagnose,1),
#    b[ID] ~ dnorm(b_group,1),
#    b_group ~ dnorm(0,1),
#    sigma ~ cauchy(0,2)
#    ),
#data=Data_2) 


# lav et plot 

# plot map posterior / model line 
plot(PitchSD ~ diagnosis, data = Data_3)+
  abline(a=coef(M_1)["a"],b=coef(M_1)["bD"])

# sample
post <- extract.samples(M_1, n=100)

# plot sample/predictive posteriors 
plot(Data_3$diagnosis, Data_3$PitchSD,
  xlim=range(Data_3$diagnosis), ylim=range(Data_3$PitchSD),
  col=rangi2,xlab="diagnosis",ylab="PitchSD")

for (i in 1:100) 
  abline(a=post$a[i],b=post$b[i],col=col.alpha("black",0.3))

#plot med link?

```


Step 4: Now re-run the model with the meta-analytic prior
- Describe and plot the estimates. Evaluate model quality

```{r}

# meta analytic prior
M_2 <- rethinking::map(
  alist(
    PitchSD ~ dnorm(mu,sigma),
    mu <- a + bD * diagnosis,
    a ~ dnorm(0,1),
    bD ~ dnorm(-0.6,0.32),
    sigma ~ dcauchy(0,2)
    ),
  data=Data_3,
  start = list(a=mean(Data_3$PitchSD),bD=0,sigma=sd(Data_3$PitchSD)))

precis(M_2)
plot(precis(M_2))

# plot map posterior / model line 
plot(PitchSD ~ diagnosis, data = Data_3)+
  abline(a=coef(M_2)["a"],b=coef(M_2)["bD"])

# sample
post <- extract.samples(M_2, n=100)

# plot sample/predictive posteriors 
plot(Data_3$diagnosis, Data_3$PitchSD,
  xlim=range(Data_3$diagnosis), ylim=range(Data_3$PitchSD),
  col=rangi2,xlab="diagnosis",ylab="PitchSD")

for (i in 1:100) 
  abline(a=post$a[i],b=post$b[i],col=col.alpha("black",0.3))

#plot med link?

```

Step 5: Compare the models
- Plot priors and posteriors of the diagnosis effect in both models
- Compare posteriors between the two models
- Compare their relative distance from truth (WAIC)
- Discuss how they compare and whether any of them is best{r}

```{r}
# step 1 and 2 adam: Prior and Posterior plot for all parameters of model 1
prior.df = data.frame(a_prior = rnorm(1e4,0,1), b_prior = rnorm(1e4,-0.6,0.32),sigma_prior = rnorm(1e4,0,2))
estimate.df = extract.samples(M_1,n = 1e4) 

df = cbind(prior.df,estimate.df)

df %>% 
  gather(key,value, a_prior:sigma) %>% 
  mutate(parameter = c(rep("a",10000),rep("b",10000), rep("sigma",10000),rep("a",10000),rep("b",10000), rep("sigma",10000))) %>% 
  ggplot(aes(value, color = key))+
  geom_density() +
  facet_wrap(~parameter)+
  xlim(-3,3)+
  ylim(0,4) +
  labs(title = "posterior parameter distributions with meta analytic priors", x = "parameter values", y ="density")

# step 1 and 2 adam: Prior and Posterior plot for all parameters of model 2
prior.df.2 = data.frame(a_prior = rnorm(1e4,0,1), b_prior = rnorm(1e4,-0.6,0.32),sigma_prior = rnorm(1e4,0,2))
estimate.df.2 = extract.samples(M_2,n = 1e4) 

df.2 = cbind(prior.df.2,estimate.df.2)

df.2 %>% 
  gather(key,value, a_prior:sigma) %>% 
  mutate(parameter = c(rep("a",10000),rep("b",10000), rep("sigma",10000),rep("a",10000),rep("b",10000), rep("sigma",10000))) %>% 
  ggplot(aes(value, color = key))+
  geom_density() +
  facet_wrap(~parameter)+
  xlim(-3,3)+
  ylim(0,4) +
  labs(title = "posterior parameter distributions with meta analytic priors", x = "parameter values", y ="density")


# step 3 WAIC
compare(M_1,M_2)

"M1"
sim1control = sim(M_1, data = Data_3[Data_3$diagnosis == '0',])
sim1schizo = sim(M_1, data = Data_3[Data_3$diagnosis == '1',])

dens(Data_3$PitchSD[Data_3$diagnosis == "1"], add = F, col = 'deepskyblue')
dens(Data_3$PitchSD[Data_3$diagnosis == '0'], add = T, col = 'deeppink')
dens(sim1control, add = T,col = 'darkred')
dens(sim1schizo, add= T, col = 'dodgerblue4')

#M2
sim2control = sim(M_2, data = Data_3[Data_3$diagnosis == '0',])
sim2schizo = sim(M_2, data = Data_3[Data_3$diagnosis == '1',])

dens(Data_3$PitchSD[Data_3$diagnosis == "1"], add = F, col = 'deepskyblue')
dens(Data_3$PitchSD[Data_3$diagnosis == '0'], add = T, col = 'deeppink')
dens(sim2control, add = T, col = 'darkred')
dens(sim2schizo, add= T, col = 'dodgerblue4')

#labels = c("Darkblue:Schizo real", "Pink:Control real", "Red: Control sim", "Lightblue: Schizo sim")



```

Step 6: Prepare a nice write up of the analysis and answer the questions at the top.

Optional step 7: how skeptical should a prior be?
- Try different levels of skepticism and compare them using WAIC.

Optional step 8: Include other predictors
- Do age, gender and education improve the model?
- Should they be main effects or interactions?

Optional step 9: Bromance magic.
- explore the bromance code below including random effects (by default with weakly informative priors)
- learn how to change the prior
- explore effects of trial, age, gender, including the appropriate random slopes
- compare the models you created using WAIC and posterior predictive check (pp_check())


```{r}

brm_out <- brm(PitchSD ~ 1 + Diagnosis  +(1|ID_unique/Study), # Outcome as a function of the predictors as in lme4. 
               data=Data, # Define the data
               family=gaussian(), # Define the family. 
               iter = 5000, warmup = 2000, cores = 4)
summary(brm_out1)
plot(brm_out1)

```

